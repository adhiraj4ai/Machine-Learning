{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhiraj4ai/Machine-Learning/blob/main/Computer%20Vision/Pytorch/lenet_with_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvXjFHZzZI_V"
      },
      "source": [
        "# Implementing LeNet using Pytorch from Scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhMRF7awZI_d"
      },
      "source": [
        "## Introduction to LeNet\n",
        "* Earliest but ground breaking model proposed by Yann Lecun and others in 1998 in their paper Gradient Based Learning Applied to Document Recoginition.\n",
        "* LeNet has five layers consisting of layers of convolutional encoder block and dense block of fully connected layers.\n",
        "* Convolutional block consists of convolutional layer, a sigmoid function and a subsequent average pooling operation.\n",
        "* Lenet's dense block consisted of three fully connected layers with sigmoid function for activation except for the last one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ1GlVIUZI_f"
      },
      "source": [
        "## Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4NcNZZaZI_g"
      },
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkzi9nSeZI_j"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lfnEvcxZI_k"
      },
      "outputs": [],
      "source": [
        "# Load the train dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='../data',\n",
        "    train = True,\n",
        "    transform= Compose([\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='../data',\n",
        "    train=False,\n",
        "    transform=Compose([\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lrKV-B6ZI_l",
        "outputId": "02c129e5-bbab-4b7c-959c-0b32f22174b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Define variables required for training\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "NUM_CLASSES = 10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Check and assign if device has GPU\n",
        "DEVICE = torch.device(\n",
        "    'cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD1HcKoAZI_o"
      },
      "outputs": [],
      "source": [
        "# Create training data loader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= BATCH_SIZE,\n",
        "    shuffle= True\n",
        ")\n",
        "\n",
        "# Create test data loader\n",
        "test_loader = DataLoader(\n",
        "    dataset= test_dataset,\n",
        "    batch_size= BATCH_SIZE,\n",
        "    shuffle= True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78EdJwpgZI_p"
      },
      "source": [
        "## Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv1sLIobZI_q",
        "outputId": "6c5587b2-2adf-4199-864b-5f573ae2d7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dimension: torch.Size([64, 1, 28, 28])\n",
            "Label dimension: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# Explore the shpae of the batch for images and labels\n",
        "for images, labels in train_loader:\n",
        "    print('Image dimension:', images.shape)\n",
        "    print('Label dimension:', labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUCzmaRjZI_r",
        "outputId": "c8d71aa1-928f-434e-c941-34b652c1c2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9098, 0.1529, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.3176, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.4706, 0.7059, 0.1529, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 0.6392, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0078, 0.6000, 0.8235, 0.1569, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.8627, 0.6392, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.1059, 0.9961, 0.6353, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.6392, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.7176, 0.9961, 0.4902, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.1804, 0.9608, 0.6392, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.7765, 0.9961, 0.2196, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.4706, 0.9961, 0.6392, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0902, 0.9059, 0.9961, 0.1137, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.6235, 0.9961, 0.4706, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.6392, 0.9961, 0.8471, 0.0627, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.6235, 0.9961, 0.2627, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549,\n",
            "          0.3373, 0.6980, 0.9725, 0.9961, 0.3569, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.6235, 0.9961, 0.3333, 0.0000, 0.0000,\n",
            "          0.0000, 0.1843, 0.1922, 0.4549, 0.5647, 0.5882, 0.9451, 0.9529,\n",
            "          0.9176, 0.7020, 0.9451, 0.9882, 0.1569, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.5882, 0.9922, 0.9294, 0.8118, 0.8118,\n",
            "          0.8118, 0.9922, 0.9961, 0.9804, 0.9412, 0.7765, 0.5608, 0.3569,\n",
            "          0.1098, 0.0196, 0.9137, 0.9804, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4667, 0.6941, 0.6941, 0.6941,\n",
            "          0.6941, 0.6941, 0.3843, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.4000, 0.9961, 0.8627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 0.9961, 0.5373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 0.9961, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 0.9961, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 1.0000, 0.3686, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 0.9961, 0.3765, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 0.9961, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6627, 1.0000, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.3765, 0.9961, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]]), 4)\n"
          ]
        }
      ],
      "source": [
        "# Explore the data arrrangement in train_dataset\n",
        "print(train_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "1keQeLH8ZI_s",
        "outputId": "fa725244-a9b6-448a-e4f7-f7e8373f6578"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot 10 images from training dataset\n",
        "figure = plt.figure(figsize = (10, 10))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    img, label = train_dataset[i]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGo9lfxdZI_t"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2HB4VuFZI_u"
      },
      "outputs": [],
      "source": [
        "# Build LeNet Model\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes) -> None:\n",
        "        super(LeNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size= 2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size= 5),\n",
        "            nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(400, 120),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(84, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        logits = self.net(X)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXsXnpD0ZI_v",
        "outputId": "b3f01d72-f0f7-4f1e-888d-e0e2d0989145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "           Sigmoid-2            [-1, 6, 28, 28]               0\n",
            "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
            "           Sigmoid-5           [-1, 16, 10, 10]               0\n",
            "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
            "           Flatten-7                  [-1, 400]               0\n",
            "            Linear-8                  [-1, 120]          48,120\n",
            "           Sigmoid-9                  [-1, 120]               0\n",
            "           Linear-10                   [-1, 84]          10,164\n",
            "          Sigmoid-11                   [-1, 84]               0\n",
            "           Linear-12                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.11\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Move the model to GPU\n",
        "lenet_model = LeNet(NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Print the model summary\n",
        "from torchsummary import summary\n",
        "summary(lenet_model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyA3DLvnZI_w"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR8w1DLAZI_x"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(lenet_model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvoLfftZI_y"
      },
      "outputs": [],
      "source": [
        "# Define train function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader)\n",
        "    \n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        # Move X, y to the device i.e. GPU if exists\n",
        "        X = X.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        # Predict for X\n",
        "        pred = model(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagate the error\n",
        "        optimizer.zero_grad()       # to reset the gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Update Model Parameters\n",
        "        optimizer.step()            \n",
        "\n",
        "        if batch % 200 == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XW8tCLOZI_z"
      },
      "outputs": [],
      "source": [
        "# Define train function\n",
        "def test(dataloader, model, loss_fn):\n",
        "    num_batches, size = len(dataloader), len(dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "    num_examples = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (X, y) in test_loader:\n",
        "            # Move X, y to the device i.e. GPU if exists\n",
        "            X = X.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            # Predict for X\n",
        "            pred = model(X)\n",
        "            # Compute loss\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # num_examples += y.sizey.size(0)\n",
        "            correct += (pred.argmax(1) == y).sum().item()\n",
        "        \n",
        "    \n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KC2tvmiZI_0",
        "outputId": "65a1b3ea-f81a-49db-f054-6995124edf7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------\n",
            "Training loss: 2.338743  [    0/  938]\n",
            "Training loss: 2.129366  [  200/  938]\n",
            "Training loss: 0.828645  [  400/  938]\n",
            "Training loss: 0.609109  [  600/  938]\n",
            "Training loss: 0.285935  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 90.3%, Avg loss: 0.338414 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------\n",
            "Training loss: 0.406498  [    0/  938]\n",
            "Training loss: 0.344672  [  200/  938]\n",
            "Training loss: 0.188355  [  400/  938]\n",
            "Training loss: 0.216724  [  600/  938]\n",
            "Training loss: 0.369100  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 94.1%, Avg loss: 0.193502 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------\n",
            "Training loss: 0.252916  [    0/  938]\n",
            "Training loss: 0.120034  [  200/  938]\n",
            "Training loss: 0.182233  [  400/  938]\n",
            "Training loss: 0.047188  [  600/  938]\n",
            "Training loss: 0.075430  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.144130 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------\n",
            "Training loss: 0.364660  [    0/  938]\n",
            "Training loss: 0.055487  [  200/  938]\n",
            "Training loss: 0.100807  [  400/  938]\n",
            "Training loss: 0.203848  [  600/  938]\n",
            "Training loss: 0.085984  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 96.5%, Avg loss: 0.111238 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------\n",
            "Training loss: 0.148719  [    0/  938]\n",
            "Training loss: 0.019625  [  200/  938]\n",
            "Training loss: 0.122711  [  400/  938]\n",
            "Training loss: 0.117112  [  600/  938]\n",
            "Training loss: 0.134679  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.090728 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------\n",
            "Training loss: 0.058241  [    0/  938]\n",
            "Training loss: 0.107794  [  200/  938]\n",
            "Training loss: 0.090824  [  400/  938]\n",
            "Training loss: 0.143652  [  600/  938]\n",
            "Training loss: 0.063077  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.080395 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------\n",
            "Training loss: 0.086613  [    0/  938]\n",
            "Training loss: 0.275173  [  200/  938]\n",
            "Training loss: 0.032333  [  400/  938]\n",
            "Training loss: 0.116366  [  600/  938]\n",
            "Training loss: 0.086409  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.069052 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------\n",
            "Training loss: 0.020176  [    0/  938]\n",
            "Training loss: 0.150121  [  200/  938]\n",
            "Training loss: 0.087665  [  400/  938]\n",
            "Training loss: 0.046685  [  600/  938]\n",
            "Training loss: 0.005000  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.059577 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------\n",
            "Training loss: 0.074376  [    0/  938]\n",
            "Training loss: 0.017605  [  200/  938]\n",
            "Training loss: 0.007849  [  400/  938]\n",
            "Training loss: 0.066855  [  600/  938]\n",
            "Training loss: 0.016339  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.058812 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------\n",
            "Training loss: 0.048457  [    0/  938]\n",
            "Training loss: 0.024737  [  200/  938]\n",
            "Training loss: 0.068279  [  400/  938]\n",
            "Training loss: 0.020890  [  600/  938]\n",
            "Training loss: 0.037954  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.052921 \n",
            "\n",
            "Epoch 11\n",
            "--------------------------\n",
            "Training loss: 0.075105  [    0/  938]\n",
            "Training loss: 0.019188  [  200/  938]\n",
            "Training loss: 0.086758  [  400/  938]\n",
            "Training loss: 0.072364  [  600/  938]\n",
            "Training loss: 0.058803  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.051894 \n",
            "\n",
            "Epoch 12\n",
            "--------------------------\n",
            "Training loss: 0.019213  [    0/  938]\n",
            "Training loss: 0.016293  [  200/  938]\n",
            "Training loss: 0.010409  [  400/  938]\n",
            "Training loss: 0.017641  [  600/  938]\n",
            "Training loss: 0.046001  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.045768 \n",
            "\n",
            "Epoch 13\n",
            "--------------------------\n",
            "Training loss: 0.042685  [    0/  938]\n",
            "Training loss: 0.044711  [  200/  938]\n",
            "Training loss: 0.035441  [  400/  938]\n",
            "Training loss: 0.014604  [  600/  938]\n",
            "Training loss: 0.054151  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.047825 \n",
            "\n",
            "Epoch 14\n",
            "--------------------------\n",
            "Training loss: 0.045015  [    0/  938]\n",
            "Training loss: 0.032964  [  200/  938]\n",
            "Training loss: 0.040984  [  400/  938]\n",
            "Training loss: 0.020482  [  600/  938]\n",
            "Training loss: 0.031648  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.050532 \n",
            "\n",
            "Epoch 15\n",
            "--------------------------\n",
            "Training loss: 0.004425  [    0/  938]\n",
            "Training loss: 0.033788  [  200/  938]\n",
            "Training loss: 0.020636  [  400/  938]\n",
            "Training loss: 0.016969  [  600/  938]\n",
            "Training loss: 0.005439  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.042469 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Execute the training\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\")\n",
        "    print(\"--------------------------\")\n",
        "    train(train_loader, lenet_model, loss_fn, optimizer)\n",
        "    test(test_loader, lenet_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfggz-_PZI_1"
      },
      "source": [
        "**Interpretation:**\n",
        "\n",
        "The base LeNet model produces the accuracy of 98.6% which is very good result with such a basic model on MNIST digit recoginiton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ3ARPijZI_2"
      },
      "source": [
        "## Modified LeNet\n",
        "We now will make suttle changes in the model and create different version of model to see if the accuracy improves with those changes.\n",
        "\n",
        "### Replace Sigmoid by RELU\n",
        "First we replace `Sigmoid` activation function with `ReLU` activation function and run our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jWwwv-KZI_3"
      },
      "outputs": [],
      "source": [
        "# Build LeNet Model by replacing Sigmoid with ReLU\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes) -> None:\n",
        "        super(LeNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride = 2),\n",
        "\n",
        "            nn.LazyConv2d(16, kernel_size= 5),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride = 2),\n",
        "            \n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oojjVhnRZI_5",
        "outputId": "3ef9480f-12f1-47f5-e59b-8903cc45e894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "# Move the model to GPU\n",
        "lenet_model2 = LeNet(NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(lenet_model2.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dF_zLE_ZI_5",
        "outputId": "1935cba7-1fa1-43b6-a6a9-8441038be1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------\n",
            "Training loss: 2.305207  [    0/  938]\n",
            "Training loss: 0.618519  [  200/  938]\n",
            "Training loss: 0.225499  [  400/  938]\n",
            "Training loss: 0.112769  [  600/  938]\n",
            "Training loss: 0.175629  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 0.119375 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------\n",
            "Training loss: 0.078671  [    0/  938]\n",
            "Training loss: 0.035422  [  200/  938]\n",
            "Training loss: 0.038941  [  400/  938]\n",
            "Training loss: 0.083587  [  600/  938]\n",
            "Training loss: 0.016197  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.065709 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------\n",
            "Training loss: 0.244407  [    0/  938]\n",
            "Training loss: 0.055963  [  200/  938]\n",
            "Training loss: 0.019988  [  400/  938]\n",
            "Training loss: 0.134522  [  600/  938]\n",
            "Training loss: 0.028622  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.047747 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------\n",
            "Training loss: 0.057976  [    0/  938]\n",
            "Training loss: 0.044037  [  200/  938]\n",
            "Training loss: 0.082574  [  400/  938]\n",
            "Training loss: 0.233676  [  600/  938]\n",
            "Training loss: 0.107661  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.040665 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------\n",
            "Training loss: 0.016405  [    0/  938]\n",
            "Training loss: 0.011838  [  200/  938]\n",
            "Training loss: 0.010723  [  400/  938]\n",
            "Training loss: 0.112337  [  600/  938]\n",
            "Training loss: 0.008559  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.039212 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------\n",
            "Training loss: 0.017094  [    0/  938]\n",
            "Training loss: 0.013449  [  200/  938]\n",
            "Training loss: 0.008677  [  400/  938]\n",
            "Training loss: 0.108202  [  600/  938]\n",
            "Training loss: 0.009182  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.033062 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------\n",
            "Training loss: 0.083766  [    0/  938]\n",
            "Training loss: 0.008570  [  200/  938]\n",
            "Training loss: 0.042453  [  400/  938]\n",
            "Training loss: 0.003107  [  600/  938]\n",
            "Training loss: 0.025990  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.027904 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------\n",
            "Training loss: 0.008307  [    0/  938]\n",
            "Training loss: 0.099780  [  200/  938]\n",
            "Training loss: 0.015450  [  400/  938]\n",
            "Training loss: 0.008636  [  600/  938]\n",
            "Training loss: 0.018742  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.037810 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------\n",
            "Training loss: 0.004907  [    0/  938]\n",
            "Training loss: 0.006370  [  200/  938]\n",
            "Training loss: 0.009404  [  400/  938]\n",
            "Training loss: 0.022386  [  600/  938]\n",
            "Training loss: 0.022025  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.030600 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------\n",
            "Training loss: 0.006072  [    0/  938]\n",
            "Training loss: 0.001042  [  200/  938]\n",
            "Training loss: 0.003938  [  400/  938]\n",
            "Training loss: 0.040078  [  600/  938]\n",
            "Training loss: 0.077891  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.039055 \n",
            "\n",
            "Epoch 11\n",
            "--------------------------\n",
            "Training loss: 0.166488  [    0/  938]\n",
            "Training loss: 0.014886  [  200/  938]\n",
            "Training loss: 0.010726  [  400/  938]\n",
            "Training loss: 0.246738  [  600/  938]\n",
            "Training loss: 0.026258  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.031511 \n",
            "\n",
            "Epoch 12\n",
            "--------------------------\n",
            "Training loss: 0.005111  [    0/  938]\n",
            "Training loss: 0.007781  [  200/  938]\n",
            "Training loss: 0.045201  [  400/  938]\n",
            "Training loss: 0.007943  [  600/  938]\n",
            "Training loss: 0.041567  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.032833 \n",
            "\n",
            "Epoch 13\n",
            "--------------------------\n",
            "Training loss: 0.000244  [    0/  938]\n",
            "Training loss: 0.005600  [  200/  938]\n",
            "Training loss: 0.002094  [  400/  938]\n",
            "Training loss: 0.003703  [  600/  938]\n",
            "Training loss: 0.001311  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.031571 \n",
            "\n",
            "Epoch 14\n",
            "--------------------------\n",
            "Training loss: 0.000665  [    0/  938]\n",
            "Training loss: 0.002156  [  200/  938]\n",
            "Training loss: 0.007875  [  400/  938]\n",
            "Training loss: 0.021125  [  600/  938]\n",
            "Training loss: 0.010735  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.035557 \n",
            "\n",
            "Epoch 15\n",
            "--------------------------\n",
            "Training loss: 0.000263  [    0/  938]\n",
            "Training loss: 0.035050  [  200/  938]\n",
            "Training loss: 0.000256  [  400/  938]\n",
            "Training loss: 0.000114  [  600/  938]\n",
            "Training loss: 0.000278  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.043863 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Execute the training\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\")\n",
        "    print(\"--------------------------\")\n",
        "    train(train_loader, lenet_model2, loss_fn, optimizer)\n",
        "    test(test_loader, lenet_model2, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqbNgMO_ZI_6"
      },
      "source": [
        "**Interpretation:**\n",
        "\n",
        "The use of ReLU function slightly improved the accuracy. We now achieved 98.8% accuracy which is expected as ReLU works better compared to Sigmoid for hidden layer activation.\n",
        "\n",
        "### Replace AvgPool by MaxPool\n",
        "Next, we replace Average pooling by Max Pooling as average pooling smooths out the image and hence the sharp feature are lost. On other hand, max pooling selects the brighter pixel from the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjW-QRZZI_7",
        "outputId": "8911a8eb-17e3-4e92-dd1b-945a5304a413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------\n",
            "Training loss: 2.300756  [    0/  938]\n",
            "Training loss: 0.173743  [  200/  938]\n",
            "Training loss: 0.144055  [  400/  938]\n",
            "Training loss: 0.064998  [  600/  938]\n",
            "Training loss: 0.074105  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.080852 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------\n",
            "Training loss: 0.038771  [    0/  938]\n",
            "Training loss: 0.022854  [  200/  938]\n",
            "Training loss: 0.050137  [  400/  938]\n",
            "Training loss: 0.133533  [  600/  938]\n",
            "Training loss: 0.091254  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.045588 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------\n",
            "Training loss: 0.041345  [    0/  938]\n",
            "Training loss: 0.030700  [  200/  938]\n",
            "Training loss: 0.101431  [  400/  938]\n",
            "Training loss: 0.004586  [  600/  938]\n",
            "Training loss: 0.011300  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.050589 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------\n",
            "Training loss: 0.035806  [    0/  938]\n",
            "Training loss: 0.017866  [  200/  938]\n",
            "Training loss: 0.006765  [  400/  938]\n",
            "Training loss: 0.002789  [  600/  938]\n",
            "Training loss: 0.088292  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.035639 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------\n",
            "Training loss: 0.084405  [    0/  938]\n",
            "Training loss: 0.006381  [  200/  938]\n",
            "Training loss: 0.006875  [  400/  938]\n",
            "Training loss: 0.006287  [  600/  938]\n",
            "Training loss: 0.011835  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.039708 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------\n",
            "Training loss: 0.010717  [    0/  938]\n",
            "Training loss: 0.092229  [  200/  938]\n",
            "Training loss: 0.126195  [  400/  938]\n",
            "Training loss: 0.021666  [  600/  938]\n",
            "Training loss: 0.006799  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.041326 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------\n",
            "Training loss: 0.031695  [    0/  938]\n",
            "Training loss: 0.011750  [  200/  938]\n",
            "Training loss: 0.080739  [  400/  938]\n",
            "Training loss: 0.003589  [  600/  938]\n",
            "Training loss: 0.035096  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.039681 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------\n",
            "Training loss: 0.106490  [    0/  938]\n",
            "Training loss: 0.004814  [  200/  938]\n",
            "Training loss: 0.000591  [  400/  938]\n",
            "Training loss: 0.003641  [  600/  938]\n",
            "Training loss: 0.010979  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.036992 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------\n",
            "Training loss: 0.040450  [    0/  938]\n",
            "Training loss: 0.072560  [  200/  938]\n",
            "Training loss: 0.133493  [  400/  938]\n",
            "Training loss: 0.005302  [  600/  938]\n",
            "Training loss: 0.038413  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.030515 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------\n",
            "Training loss: 0.000217  [    0/  938]\n",
            "Training loss: 0.007458  [  200/  938]\n",
            "Training loss: 0.006006  [  400/  938]\n",
            "Training loss: 0.001375  [  600/  938]\n",
            "Training loss: 0.245885  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.034448 \n",
            "\n",
            "Epoch 11\n",
            "--------------------------\n",
            "Training loss: 0.000381  [    0/  938]\n",
            "Training loss: 0.007227  [  200/  938]\n",
            "Training loss: 0.036869  [  400/  938]\n",
            "Training loss: 0.020514  [  600/  938]\n",
            "Training loss: 0.001000  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.042069 \n",
            "\n",
            "Epoch 12\n",
            "--------------------------\n",
            "Training loss: 0.002998  [    0/  938]\n",
            "Training loss: 0.012582  [  200/  938]\n",
            "Training loss: 0.005311  [  400/  938]\n",
            "Training loss: 0.024369  [  600/  938]\n",
            "Training loss: 0.052775  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.037573 \n",
            "\n",
            "Epoch 13\n",
            "--------------------------\n",
            "Training loss: 0.000431  [    0/  938]\n",
            "Training loss: 0.000216  [  200/  938]\n",
            "Training loss: 0.000336  [  400/  938]\n",
            "Training loss: 0.000967  [  600/  938]\n",
            "Training loss: 0.043697  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.055378 \n",
            "\n",
            "Epoch 14\n",
            "--------------------------\n",
            "Training loss: 0.005848  [    0/  938]\n",
            "Training loss: 0.002282  [  200/  938]\n",
            "Training loss: 0.001197  [  400/  938]\n",
            "Training loss: 0.000106  [  600/  938]\n",
            "Training loss: 0.003792  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.039883 \n",
            "\n",
            "Epoch 15\n",
            "--------------------------\n",
            "Training loss: 0.011642  [    0/  938]\n",
            "Training loss: 0.002860  [  200/  938]\n",
            "Training loss: 0.002613  [  400/  938]\n",
            "Training loss: 0.001147  [  600/  938]\n",
            "Training loss: 0.000406  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.041455 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build LeNet Model with Max Pooling\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes) -> None:\n",
        "        super(LeNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= 2, stride=2),\n",
        "            \n",
        "            nn.LazyConv2d(16, kernel_size= 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Move the model to GPU\n",
        "lenet_model3 = LeNet(NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(lenet_model3.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "# Execute the training\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\")\n",
        "    print(\"--------------------------\")\n",
        "    train(train_loader, lenet_model3, loss_fn, optimizer)\n",
        "    test(test_loader, lenet_model3, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgoSwzB1ZI_9"
      },
      "source": [
        "**Interpretation**:\n",
        "With the replacement of AvgPool by MaxPool, we notices the improvement of 0.01% in the accuracy. This is very small but noticeable improvement.\n",
        "\n",
        "\n",
        "### Batch Normalize the layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnChCnuRZI_9",
        "outputId": "95e9fa59-9550-4320-dc9b-ece7259f3aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------\n",
            "Training loss: 2.341679  [    0/  938]\n",
            "Training loss: 0.140223  [  200/  938]\n",
            "Training loss: 0.039875  [  400/  938]\n",
            "Training loss: 0.018332  [  600/  938]\n",
            "Training loss: 0.105512  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.063741 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------\n",
            "Training loss: 0.066084  [    0/  938]\n",
            "Training loss: 0.031427  [  200/  938]\n",
            "Training loss: 0.009188  [  400/  938]\n",
            "Training loss: 0.199526  [  600/  938]\n",
            "Training loss: 0.050924  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.055864 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------\n",
            "Training loss: 0.097901  [    0/  938]\n",
            "Training loss: 0.044612  [  200/  938]\n",
            "Training loss: 0.064392  [  400/  938]\n",
            "Training loss: 0.025873  [  600/  938]\n",
            "Training loss: 0.040664  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.036235 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------\n",
            "Training loss: 0.002231  [    0/  938]\n",
            "Training loss: 0.042320  [  200/  938]\n",
            "Training loss: 0.009110  [  400/  938]\n",
            "Training loss: 0.006137  [  600/  938]\n",
            "Training loss: 0.095520  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.037235 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------\n",
            "Training loss: 0.093074  [    0/  938]\n",
            "Training loss: 0.007173  [  200/  938]\n",
            "Training loss: 0.002897  [  400/  938]\n",
            "Training loss: 0.065615  [  600/  938]\n",
            "Training loss: 0.059051  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.039920 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------\n",
            "Training loss: 0.006439  [    0/  938]\n",
            "Training loss: 0.004632  [  200/  938]\n",
            "Training loss: 0.008208  [  400/  938]\n",
            "Training loss: 0.029597  [  600/  938]\n",
            "Training loss: 0.004306  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.035807 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------\n",
            "Training loss: 0.011829  [    0/  938]\n",
            "Training loss: 0.012398  [  200/  938]\n",
            "Training loss: 0.011332  [  400/  938]\n",
            "Training loss: 0.107336  [  600/  938]\n",
            "Training loss: 0.001398  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.034603 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------\n",
            "Training loss: 0.012028  [    0/  938]\n",
            "Training loss: 0.026264  [  200/  938]\n",
            "Training loss: 0.002752  [  400/  938]\n",
            "Training loss: 0.008243  [  600/  938]\n",
            "Training loss: 0.027863  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.038048 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------\n",
            "Training loss: 0.005848  [    0/  938]\n",
            "Training loss: 0.031740  [  200/  938]\n",
            "Training loss: 0.000410  [  400/  938]\n",
            "Training loss: 0.036893  [  600/  938]\n",
            "Training loss: 0.000385  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.031730 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------\n",
            "Training loss: 0.002497  [    0/  938]\n",
            "Training loss: 0.008553  [  200/  938]\n",
            "Training loss: 0.002996  [  400/  938]\n",
            "Training loss: 0.003246  [  600/  938]\n",
            "Training loss: 0.038521  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.039831 \n",
            "\n",
            "Epoch 11\n",
            "--------------------------\n",
            "Training loss: 0.008909  [    0/  938]\n",
            "Training loss: 0.007658  [  200/  938]\n",
            "Training loss: 0.000794  [  400/  938]\n",
            "Training loss: 0.000443  [  600/  938]\n",
            "Training loss: 0.012852  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.032635 \n",
            "\n",
            "Epoch 12\n",
            "--------------------------\n",
            "Training loss: 0.012705  [    0/  938]\n",
            "Training loss: 0.019280  [  200/  938]\n",
            "Training loss: 0.000799  [  400/  938]\n",
            "Training loss: 0.001789  [  600/  938]\n",
            "Training loss: 0.000291  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.032423 \n",
            "\n",
            "Epoch 13\n",
            "--------------------------\n",
            "Training loss: 0.000171  [    0/  938]\n",
            "Training loss: 0.003398  [  200/  938]\n",
            "Training loss: 0.000061  [  400/  938]\n",
            "Training loss: 0.006314  [  600/  938]\n",
            "Training loss: 0.022359  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.027634 \n",
            "\n",
            "Epoch 14\n",
            "--------------------------\n",
            "Training loss: 0.000110  [    0/  938]\n",
            "Training loss: 0.000108  [  200/  938]\n",
            "Training loss: 0.000511  [  400/  938]\n",
            "Training loss: 0.062018  [  600/  938]\n",
            "Training loss: 0.000317  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.042112 \n",
            "\n",
            "Epoch 15\n",
            "--------------------------\n",
            "Training loss: 0.000819  [    0/  938]\n",
            "Training loss: 0.017399  [  200/  938]\n",
            "Training loss: 0.004238  [  400/  938]\n",
            "Training loss: 0.000125  [  600/  938]\n",
            "Training loss: 0.000743  [  800/  938]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.037975 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build LeNet Model with Batch Normalization\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes) -> None:\n",
        "        super(LeNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= 2, stride=2),\n",
        "            \n",
        "            nn.LazyConv2d(16, kernel_size= 5),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Move the model to GPU\n",
        "lenet_model4 = LeNet(NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(lenet_model4.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "# Execute the training\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\")\n",
        "    print(\"--------------------------\")\n",
        "    train(train_loader, lenet_model4, loss_fn, optimizer)\n",
        "    test(test_loader, lenet_model4, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caN92erwZI__"
      },
      "source": [
        "**Interpretation**\n",
        "There is very small improvement noticed in the performance with the introduction of batch normalization in the network.\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "Finally, we can conclude from above experimentation that the LeNet performance improves by\n",
        "1. replacing the sigmoid by ReLU function.\n",
        "2. replacing the AvgPool (Average Pooling) by MaxPool (Max pooling)\n",
        "3. introducing the batchnormalization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8187e0322037ab0269efe25fdc1a3b0e2caf5b907057e06f3a7764b0ae9a121b"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}